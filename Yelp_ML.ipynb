{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip, urllib2, re\n",
    "\n",
    "import dill as pickle\n",
    "import simplejson as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections as collections\n",
    "\n",
    "from scipy import stats\n",
    "from StringIO import StringIO\n",
    "\n",
    "from sklearn import datasets, decomposition, linear_model, grid_search, utils, preprocessing, cross_validation, neighbors, ensemble\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING A VENUE'S POPULARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to use the Yelp dataset to predict a new venue's popularity from information available when the venue opens.  I do this by testing various machine learning algorithms on a Yelp dataset.  It contains meta data about the venue (where it is located, the type of food served, etc), and a star rating.\n",
    "\n",
    "There are two parts to this project. The first is importing the data, and splitting it into test and training sets. The second is building and testing predictive models. I use the scikit_learn for the second part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HandleGZippedJSON:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.json_data = None\n",
    " \n",
    "    def run(self):\n",
    "        request = urllib2.Request(self.url)\n",
    "        request.add_header('Accept-encoding', 'gzip')\n",
    "        opener = urllib2.build_opener()\n",
    "        f = opener.open(request)\n",
    "        c_data = f.read()\n",
    "        c_stream = StringIO(c_data)\n",
    "        gzipper = gzip.GzipFile(fileobj=c_stream)\n",
    "        data = gzipper.read()\n",
    "        \n",
    "        output = data.splitlines()\n",
    "        datastr=[]\n",
    "\n",
    "        for lines in output:\n",
    "            try:\n",
    "                r=json.loads(lines)\n",
    "                datastr.append(r)\n",
    "            except ValueError:  # includes simplejson.decoder.JSONDecodeError\n",
    "                print 'Decoding JSON has failed'\n",
    "                pass\n",
    "        \n",
    "        return datastr\n",
    "    \n",
    "fileurl=\"http://thedataincubator.s3.amazonaws.com/coursedata/mldata/yelp_train_academic_dataset_business.json.gz\"\n",
    "out=HandleGZippedJSON(fileurl)\n",
    "xfile=out.run()\n",
    "yout=[]\n",
    "for records in xfile:\n",
    "    yout.append(records['stars'])\n",
    "\n",
    "    \n",
    "#splitting dataset into testing and training sets\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(xfile, yout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I test the predictive power of various features using scikit.learn's Transformer and Estimator classes, FeatureUnion (which allows me to chain transformers together), and Pipelines (which allows me to chain together FeatureUnion, Estimators, other Pipelines, etc -- the whole workflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,model,name):\n",
    "        self.model=model\n",
    "        self.name=name\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [self.model.predict(x) for x in X]\n",
    "\n",
    "class ColumnSelector(TransformerMixin):\n",
    "  \n",
    "    def __init__(self,namecol):\n",
    "        self.namecol=namecol\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, data):\n",
    "        if type(data) is list:\n",
    "            df = pd.DataFrame(data)\n",
    "            D=df[self.namecol]\n",
    "        elif type(data) is dict:\n",
    "            df = pd.DataFrame(columns=[self.namecol], index=['x'])\n",
    "            df.loc['x'] = pd.Series({self.namecol:data[self.namecol]})\n",
    "            D=df[self.namecol]\n",
    "\n",
    "        return D\n",
    "\n",
    "class DictList(TransformerMixin):\n",
    "  \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, data):\n",
    "        def flatten(d, parent_key='', sep='_'):\n",
    "            items = []\n",
    "            for k, v in d.items():\n",
    "                new_key = parent_key + sep + k if parent_key else k\n",
    "                if isinstance(v, collections.MutableMapping):\n",
    "                    items.extend(flatten(v, new_key, sep=sep).items())\n",
    "                else:\n",
    "                    items.append((new_key, v))\n",
    "            return dict(items)\n",
    "        \n",
    "        dictlist=[]\n",
    "        for dicts in data:\n",
    "            dictlist.append(flatten(dicts))\n",
    "\n",
    "        return dictlist\n",
    "    \n",
    "class NestExtractor(TransformerMixin):\n",
    "  \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, data):\n",
    "        dictlist=[]\n",
    "        for dicts in data:\n",
    "            for key, value in dicts.items():\n",
    "                if type(dicts[key]) is bool:\n",
    "                    dicts[key]=int(dicts[key])\n",
    "                if type(dicts[key]) is str:\n",
    "                    keyn = key + \"_\" + dicts[key]\n",
    "                    dicts[keyn]=1\n",
    "                    del dicts[key]\n",
    "            dictlist.append(dicts)\n",
    "        return dictlist\n",
    "    \n",
    "class CategoriesExtractor(TransformerMixin):\n",
    "  \n",
    "    def __init__(self):\n",
    "        self.t=DictVectorizer()\n",
    "\n",
    "    def fit(self, data, y=None):        \n",
    "        #extract categories from data\n",
    "        if type(data) is list:\n",
    "            df = pd.DataFrame(data)\n",
    "            D=df['categories']\n",
    "        elif type(data) is dict:\n",
    "            df = pd.DataFrame(columns=['categories'], index=['x'])\n",
    "            df.loc['x'] = pd.Series({'categories':data['categories']})\n",
    "        D=df['categories']\n",
    "        dictlist=[]\n",
    "        for lines in D:\n",
    "            x={}\n",
    "            for words in lines:\n",
    "                x[words]=1\n",
    "            dictlist.append(x)\n",
    "        self.t.fit(dictlist)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        #extract categories from data\n",
    "        if type(data) is list:\n",
    "            df = pd.DataFrame(data)\n",
    "            D=df['categories']\n",
    "        elif type(data) is dict:\n",
    "            df = pd.DataFrame(columns=['categories'], index=['x'])\n",
    "            df.loc['x'] = pd.Series({'categories':data['categories']})\n",
    "            D=df['categories']\n",
    "        else:\n",
    "            df= pd.DataFrame(data)\n",
    "            D=df['categories']\n",
    "        dictlist2=[]\n",
    "        for lines in D:\n",
    "            x={}\n",
    "            for words in lines:\n",
    "                x[words]=1\n",
    "            dictlist2.append(x)\n",
    "        X=self.t.transform(dictlist2)\n",
    "        return X\n",
    "    \n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class LatLongExtractor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #extract columns from data                                                                  \n",
    "        if type(X) is list:\n",
    "            df = pd.DataFrame(X)\n",
    "            df.drop(['attributes','business_id','categories','city','full_address','hours','name','\\\n",
    "neighborhoods','open','review_count','stars','state','type'],inplace=True,axis=1)\n",
    "            return df\n",
    "        elif type(X) is dict:\n",
    "            df = pd.DataFrame(columns=['latitude','longitude'], index=['x'])\n",
    "            df.loc['x'] = pd.Series({'latitude':X['latitude'], 'longitude':X['longitude']})\n",
    "            return df\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "    def __init__(self, namecols):\n",
    "        self.namecols = namecols\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args, **transform_params):\n",
    "        if isinstance(X,dict):\n",
    "            return [X[arg] for arg in self.namecols]\n",
    "        elif isinstance(X,list):\n",
    "            return [[rec[arg] for arg in self.namecols] for rec in X]\n",
    "        \n",
    "class LinearRegressor2(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.linear_regression = linear_model.LinearRegression()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.linear_regression.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Xy=self.linear_regression.predict(X)\n",
    "        if type(Xy) is list:\n",
    "            Xyz=Xy\n",
    "        elif type(Xy) is np.ndarray:\n",
    "            Xyz=[]\n",
    "            for record in Xy:\n",
    "                frecord=float(record)\n",
    "                Xyz.append(frecord)\n",
    "        if len(Xyz)<2:\n",
    "            Xyz=Xyz[0]\n",
    "        return Xyz\n",
    "\n",
    "class EnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.random_forest = ensemble.RandomForestRegressor(min_samples_leaf=20).fit(X, y)\n",
    "        return self.random_forest\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Xy=self.random_forest.predict(X)\n",
    "        if type(Xy) is list:\n",
    "            Xyz=Xy\n",
    "        else:\n",
    "            Xyz=Xy.tolist()\n",
    "            #Xyz=Xyz[0]\n",
    "        return Xyz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) I first test the predictive power of the city of the venue (perhaps some cities have higher average ratings than others). I get a very low score, indicating that this is not predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00492804346835\n"
     ]
    }
   ],
   "source": [
    "#BASED ON CITY\n",
    "\n",
    "class EstimatorKL(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        infilex=X\n",
    "        sortbyx='city'\n",
    "        df = pd.DataFrame(infilex)\n",
    "        grouped=df.groupby(sortbyx, group_keys=True)\n",
    "        keylist=[]\n",
    "        for k, gp in grouped:\n",
    "            keylist.append(str(k))\n",
    "        cities_avg={}\n",
    "        for keysl in keylist:\n",
    "            cities_avg.update({keysl: grouped.get_group(keysl)['stars'].mean()})\n",
    "        self.ds=cities_avg\n",
    "        return self.ds\n",
    "    \n",
    "    def predict(self, X):\n",
    "        akey=[]\n",
    "        for record in X:\n",
    "            try:\n",
    "                akey.append(self.ds[record['city']])\n",
    "            except:\n",
    "                akey.append(3.67)\n",
    "        return akey\n",
    "    \n",
    "estimator = EstimatorKL()  # initialize\n",
    "estimator.fit(x_train,y_train)  # fit data\n",
    "print estimator.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Perhaps the city-based model isn't fine-grained enough. Therefore, I use both a Random Forests and a K-Nearest Neighbors model on the location of the venue (latitude/longitude) to test how predictive neighborhoods are. The scores show that the predictive power of location, while better than city, is still very weak. Moreover, both the Random Forest regressor and the KNN regressor seemed to have about the same predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The score for the Random Forest Regressor is: \n",
      "\n",
      "0.0173717385617\n",
      "\n",
      "\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          n_neighbors=59, p=2, weights='uniform')\n",
      "\n",
      " The score for the KNN Regressor is: \n",
      "\n",
      "0.0179731781387\n"
     ]
    }
   ],
   "source": [
    "#BASED ON LATITUDE AND LONGITUDE\n",
    "\n",
    "#RandomForest Regressor\n",
    "pipeline = Pipeline([\n",
    "    ('latlong', LatLongExtractor()),\n",
    "    ('rforest', ensemble.RandomForestRegressor(min_samples_leaf=20))\n",
    "                    ])\n",
    "\n",
    "pipeline.fit(x_train,y_train)\n",
    "print \" The score for the Random Forest Regressor is: \\n\"\n",
    "print pipeline.score(x_test,y_test)\n",
    "\n",
    "#K-Nearest Neighbors Regressor\n",
    "\n",
    "x_latlong = [[xa['latitude'],xa['longitude']] for xa in x_train]\n",
    "cv = cross_validation.ShuffleSplit(len(y_train), n_iter=20, test_size=0.2, random_state=40)\n",
    "param_grid = { \"n_neighbors\": range(4, 100, 5) }\n",
    "nearest_neighbors_cv = grid_search.GridSearchCV(neighbors.KNeighborsRegressor(), param_grid=param_grid, cv=cv)\n",
    "nearest_neighbors_cv.fit(x_latlong,y_train)\n",
    "print(\"\\n\")\n",
    "print(nearest_neighbors_cv.best_estimator_)\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('latlong', LatLongExtractor()),\n",
    "    ('knn', nearest_neighbors_cv.best_estimator_)\n",
    "                    ])\n",
    "\n",
    "pipeline2.fit(x_train,y_train)\n",
    "print \"\\n The score for the KNN Regressor is: \\n\"\n",
    "print pipeline2.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Venues have categories with varying degrees of specificity, such as:\n",
    "    [American (Traditional), Restaurants]\n",
    "    [Restaurants]\n",
    "    [Doctors, Health & Medical]\n",
    "\n",
    "For this part, (a) I built a custom transformer that massages the data and feeds it into the DictVectorizer, generating a large matrix via One-Hot-Encoding. (b) I then used the TfidfTransformer to normalize categories such that the influence of common, high frequency words (i.e. restaurants) was lessened and the influence of uncommon words increased. (c) I then ran a ridge regression. I didn't use a nonlinear predictor such as KNN for these category-features because of the high dimensionality; and I didn't carry out a linear regression because of concerns about overfitting.\n",
    "\n",
    "The score (.17) shows that categories are weak predictors of Yelp Ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167279104669\n"
     ]
    }
   ],
   "source": [
    "#BASED ON CATEGORIES\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('catextract', CategoriesExtractor()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('estimate', linear_model.Ridge())\n",
    "                            ])\n",
    "\n",
    "pipeline.fit(x_train,y_train)\n",
    "print pipeline.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now check the predictive power of venue attributes (wifi,good for groups, etc.).\n",
    "\n",
    "(1) Attributes are a mixture of nested and unnested:   \n",
    "    {'Attire': 'casual',\n",
    "     'Accepts Credit Cards': True,\n",
    "     'Ambience': {'casual': False, 'classy': False }}\n",
    "        \n",
    "I flatten the nested attributes and transform them into a features matrix via one-hot encoding, so that the above becomes:\n",
    "\n",
    "    {'Attire_casual' : 1,\n",
    "     'Accepts Credit Cards': 1,\n",
    "     'Ambience_casual': 0,\n",
    "     'Ambience_classy': 0 }\n",
    "\n",
    "(2) I repeat the steps above -- running it through a dictionary vectorizer, tfidf transformer, and finally, a Ridge regression.\n",
    "\n",
    "As the score shows, venue attributes are a very weak predictor of Yelp Ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0761642437615\n"
     ]
    }
   ],
   "source": [
    "#BASED ON ATTRIBUTES\n",
    "\n",
    "pipeline =  Pipeline([\n",
    "    ('att_extractor', ColumnSelector('attributes')),\n",
    "    ('unnestor', DictList()),\n",
    "    ('nobooler', NestExtractor()),\n",
    "    ('dictv', DictVectorizer(sparse='FALSE')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('estimate', linear_model.Ridge())\n",
    "                            ])\n",
    "\n",
    "pipeline.fit(x_train,y_train)\n",
    "print pipeline.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have three weak predictors: location, categories and attributes. I will now build a model combining these predictors using scikitlearn's FeatureUnion to combine. Since FeatureUnion only run on Transformers, I will have to transform the final estimators for each of the predictors into Transformers using the ModelTransformer() class created above.\n",
    "\n",
    "While this combined model (score: .15) fares better than just the latlong (score: .02) or the attributes model (score: .08), it is outscored by the Categories predictor (score: .17) alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.154383437543\n"
     ]
    }
   ],
   "source": [
    "pipeline_all = Pipeline([\n",
    "        ('festimators', FeatureUnion([\n",
    "                    ('categories', Pipeline([\n",
    "                                ('catextract', CategoriesExtractor()),\n",
    "                                ('tfidf', TfidfTransformer()),\n",
    "                                ('lregcat', ModelTransformer(linear_model.Ridge(),'cat'))                              \n",
    "                            ])),\n",
    "                    ('latlong', Pipeline([\n",
    "                                ('latlong', ColumnExtractor(['latitude','longitude'])),\n",
    "                                ('rforest', ModelTransformer(ensemble.RandomForestRegressor(min_samples_leaf=20),'rforest'))\n",
    "                            ])),\n",
    "                    ('attributes', Pipeline([\n",
    "                                ('att_extractor', ColumnSelector('attributes')),\n",
    "                                ('unnestor', DictList()),\n",
    "                                ('nobooler', NestExtractor()),\n",
    "                                ('dictv', DictVectorizer(sparse='FALSE')),\n",
    "                                ('tfidf', TfidfTransformer()),\n",
    "                                ('linearize', ModelTransformer(linear_model.Ridge(),'att'))\n",
    "                            ]))\n",
    "                ])),\n",
    "        ('estimate', linear_model.LinearRegression())\n",
    "    ])\n",
    "\n",
    "pipeline_all.fit(x_train,y_train)\n",
    "print pipeline_all.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will explore whether running NLP on Yelp yields better predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
