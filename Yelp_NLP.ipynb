{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import gzip\n",
    "import simplejson as json\n",
    "import urllib2\n",
    "from StringIO import StringIO\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "import re\n",
    "import pickle as pk\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk.tokenize as tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn import cross_validation, grid_search\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING A VENUE'S RATING USING NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My objective is to accurately extract the sentiment (positive or negative) from Yelp review text, and predict the Yelp star ratings. There are over one million records in this dataset, and one of the constraints I faced was producing a model streamlined enough to go under Heroku's memory limits. For this reason, I used HashVectorizer when I could (in place of CountVectorizer), and stayed away from memory intensive search and validation procedures (such as GridSearchCV). However, even within these limitations, I was able to build models orders magnitudes more accurate than those in the previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n",
      "{'votes': {'funny': 0, 'useful': 2, 'cool': 1}, 'user_id': 'Xqd0DzHaiyRqVH3WRG7hzg', 'review_id': '15SdjuK7DmYqUAj6rjGowg', 'text': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\", 'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA', 'stars': 5, 'date': '2007-05-17', 'type': 'review'}\n"
     ]
    }
   ],
   "source": [
    "class HandleGZippedJSON:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.json_data = None\n",
    " \n",
    "    def run(self):\n",
    "        request = urllib2.Request(self.url)\n",
    "        request.add_header('Accept-encoding', 'gzip')\n",
    "        opener = urllib2.build_opener()\n",
    "        f = opener.open(request)\n",
    "        c_data = f.read()\n",
    "        c_stream = StringIO(c_data)\n",
    "        gzipper = gzip.GzipFile(fileobj=c_stream)\n",
    "        data = gzipper.read()\n",
    "        \n",
    "        output = data.splitlines()\n",
    "        datastr=[]\n",
    "\n",
    "        for lines in output:\n",
    "            try:\n",
    "                r=json.loads(lines)\n",
    "                datastr.append(r)\n",
    "            except ValueError:  # includes simplejson.decoder.JSONDecodeError\n",
    "                print 'Decoding JSON has failed'\n",
    "                pass\n",
    "        \n",
    "        return datastr\n",
    "\n",
    "fileurl=\"http://thedataincubator.s3.amazonaws.com/coursedata/mldata/yelp_train_academic_dataset_review.json.gz\"\n",
    "out=HandleGZippedJSON(fileurl)\n",
    "xfile=out.run()\n",
    "df = pd.DataFrame(xfile)\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(xfile,df['stars'],test_size=0.2)\n",
    "print(len(xfile))\n",
    "print(xfile[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I create a bag of words. I then build a (regularized) linear model based on the count of the words in each document (review).  \n",
    "\n",
    "Some comments:\n",
    "\n",
    "1) Tokenization is important for good performance, but it is also the most expensive step. For this reason, I vectorize as the first step.\n",
    "\n",
    "2) 'CountVectorizer' has to memorize the mapping between words and the index to which it is assigned.  This is linear in the size of the vocabulary.  'HashingVectorizer' doesn't have to remember this mapping and leads to much smaller models. For this reason, for the first two parts of this problem (when predicting rating) I use the hashing vectorizer.\n",
    "\n",
    "3) I used a Ridge Regression for predicting ratings. Stochastic Gradient Descent [`SGDRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) was also an option, but the results were much poorer (score of .46 vs .63)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(TransformerMixin):\n",
    "  \n",
    "    def __init__(self,namecol):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        self.namecol=namecol\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        return self\n",
    "        \n",
    "    def transform(self, data):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        \n",
    "        if type(data) is list:\n",
    "            df = pd.DataFrame(data)\n",
    "            D=df[self.namecol]\n",
    "        elif type(data) is dict:\n",
    "            df = pd.DataFrame(columns=[self.namecol], index=['x'])\n",
    "            df.loc['x'] = pd.Series({self.namecol:data[self.namecol]})\n",
    "            D=df[self.namecol]\n",
    "        return D\n",
    "        \n",
    "class RidgeRegressor2(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import datasets, linear_model, utils, preprocessing, cross_validation, neighbors, ensemble\n",
    "        self.ridge_regression = linear_model.Ridge().fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn import datasets, linear_model, utils, preprocessing, cross_validation, neighbors, ensemble\n",
    "\n",
    "        Xy=self.ridge_regression.predict(X)\n",
    "        if type(Xy) is list:\n",
    "            Xyz=Xy\n",
    "        elif type(Xy) is np.ndarray:\n",
    "            Xyz=[]\n",
    "            for record in Xy:\n",
    "                frecord=float(record)\n",
    "                Xyz.append(frecord)\n",
    "        if len(Xyz)<2:\n",
    "            Xyz=Xyz[0]\n",
    "        return Xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.627198492197\n"
     ]
    }
   ],
   "source": [
    "### JUST USING ONE-GRAMS ####\n",
    "\n",
    "mypipeline=Pipeline([\n",
    "  ('text_extractor', ColumnSelector('text')),\n",
    "  ('hvect', HashingVectorizer(norm='l2',stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "  ('ridgefit', RidgeRegressor2())\n",
    "])\n",
    "\n",
    "mypipeline.fit(x_train,y_train)\n",
    "\n",
    "print(mypipeline.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.662585007173\n"
     ]
    }
   ],
   "source": [
    "#### ALSO USING BIGRAMS ####\n",
    "\n",
    "mypipeline2=Pipeline([\n",
    "    ('text_extractor', ColumnSelector('text')),\n",
    "    ('hvect', HashingVectorizer(norm='l2', ngram_range=(1, 2), stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "    ('ridgefit', RidgeRegressor2())\n",
    "])\n",
    "\n",
    "mypipeline2.fit(x_train,y_train)\n",
    "\n",
    "print(mypipeline2.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOOD BIGRAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I identify word pairs in restaurant reviews that are more likely than the individual words alone. These might be strongly indicative of \"foodie\" type words that you might expect to find in a yelp review such as \"huevos rancheros\".\n",
    "\n",
    "For this part, we need to use a CountVectorizer instead of a HashVectorizer, which will slow down our calculations.\n",
    "\n",
    "We can find word pairs that are unlikely to occur consecutively based on the underlying probability of their words.\n",
    "\n",
    "Mathematically, if p(w) is the probability of a word w and p(w1w2) is the probability of the bigram w1w2, then we want to look at word pairs w1w2 where the statistic\n",
    "\n",
    "p(w1w2)/p(w1)/p(w2)\n",
    "\n",
    "is high.\n",
    "\n",
    "Because this metric is problematic when p(w_1) and/or p(w_2) are small, I add a Bayesian prior to all the word probabilities.\n",
    "\n",
    "There are two steps to this analysis.\n",
    "\n",
    "First, I load in the previous Yelp data set to idenitify which businesses are restaurants, and do an SQL style join on my two pandas dataframes. This allows me to select reviews that only correspond to restaurants and by extension special food bigrams.\n",
    "\n",
    "Then, I clean and parse the gathered data and run the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING AND MERGING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n",
      "634495\n"
     ]
    }
   ],
   "source": [
    "#import and merge data from previous dataset to identify restaurants\n",
    "\n",
    "fileurl2=\"http://thedataincubator.s3.amazonaws.com/coursedata/mldata/yelp_train_academic_dataset_business.json.gz\"\n",
    "out=HandleGZippedJSON(fileurl2)\n",
    "xfile_rests=out.run()\n",
    "dfrest=pd.DataFrame(xfile_rests)\n",
    "xrests=list(myrests['business_id'] for myrests in xfile_rests if ('Restaurants' in myrests['categories']) or ('Food' in myrests['categories']))\n",
    "print(len(xfile))\n",
    "xfile2 = [review for review in xfile if review['business_id'] in xrests]\n",
    "print(len(xfile2))\n",
    "dfonlyrests=pd.DataFrame(xfile2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING, PROCESSING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviewtext=dfonlyrests['text']\n",
    "estopwords=stopwords.words('english')\n",
    "\n",
    "xonegramst = CountVectorizer(ngram_range=(1,1),stop_words=estopwords)\n",
    "xonegrams = xonegramst.fit_transform(reviewtext)\n",
    "xbigramst = CountVectorizer(ngram_range=(2,2),stop_words=estopwords)\n",
    "xbigrams = xbigramst.fit_transform(reviewtext)\n",
    "all_onegrams=xonegramst.get_feature_names()\n",
    "all_bigrams=xbigramst.get_feature_names()\n",
    "\n",
    "tot_words=xonegrams.sum() #total words in corpus (~50 million)\n",
    "unique_words=xonegramst.get_feature_names()\n",
    "tot_unique=len(unique_words) #total unique words in corpus (~322k)\n",
    "new_tot_words=tot_words+tot_unique\n",
    "wordcount_list=np.array(xonegrams.sum(axis=0))[0] #array of occurrances of each word in corpus\n",
    "wordloc=xonegramst.vocabulary_ #location of each unique word in list\n",
    "wc_list = [wordcount_list[wordloc[key]] for key in all_onegrams] #occurrances of particular unique word\n",
    "\n",
    "###BIGRAMS###\n",
    "unique_biwords=xbigramst.get_feature_names()\n",
    "bi_keys_split = [re.split('\\s',key) for key in unique_biwords]\n",
    "biwordcount_list=np.array(xbigrams.sum(axis=0))[0]\n",
    "biwordloc=xbigramst.vocabulary_\n",
    "bi_wc_list=[biwordcount_list[biwordloc[key]] for key in unique_biwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'pel meni', u'laan xang', u'f_5_unx wrafcxuakbzrdw', u'roka akor', u'grana padano', u'chicha morada', u'dol sot', u'innis gunn', u'himal chuli', u'hodge podge', u'ferrero rocher', u'hoity toity', u'celine dion', u'perrier jouet', u'mille feuille', u'luc lac', u'ore ida', u'hy vee', u'riff raff', u'reina pepiada', u'alain ducasse', u'cien agaves', u'rustler rooste', u'ezzyujdouig4p gyb3pv_a', u'khai hoan', u'deja vu', u'dueling pianos', u'feng shui', u'nooks crannies', u'hu tieu', u'nanay gloria', u'hors oeuvres', u'mccormick schmick', u'leaps bounds', u'haagen dazs', u'homer simpson', u'barnes noble', u'connective tissue', u'tutti santi', u'palo alto', u'marche bacchus', u'fru fru', u'knick knacks', u'bla bla', u'ak yelpcdn', u'jap chae', u'itty bitty', u'nuoc mam', u'khao soi', u'horny toad', u'haricot vert', u'bai thong', u'highs lows', u'holyrood 9a', u'porta alba', u'gulab jamun', u'shiner bock', u'molecular gastronomy', u'ropa vieja', u'org wiki', u'yadda yadda', u'visa mastercard', u'rx boiler', u'lloyd wright', u'dun dun', u'womp womp', u'pura vida', u'190 octane', u'haricot verts', u'lou malnati', u'ritz carlton', u'harley davidson', u'rula bula', u'turo turo', u'estiatorio milos', u'snicker doodle', u'bubba gump', u'tammie coe', u'salo salo', u'sous vide', u'quattro formaggi', u'dac biet', u'fritto misto', u'penn teller', u'vice versa', u'hob nobs', u'dolce vita', u'ba reeba', u'kao tod', u'goi cuon', u'malai kofta', u'lomo saltado', u'fra diavolo', u'wells fargo', u'wayne gorsek', u'jw marriott', u'upward projects', u'bradley ogden', u'chino bandido', u'baskin robbins']\n"
     ]
    }
   ],
   "source": [
    "def get_probs(xgrams,xgramst):\n",
    "    wordloc=xgramst.vocabulary_\n",
    "    unique_words=xgramst.get_feature_names()\n",
    "    tot_words=xgrams.sum() #total words in corpus (~50 million)\n",
    "    wordcount_list=np.array(xgrams.sum(axis=0))[0]\n",
    "    prob_word={}\n",
    "    for xword in unique_words:\n",
    "        prob_word[xword] = float((wordcount_list[wordloc[xword]]) + 5)\n",
    "    return prob_word\n",
    "\n",
    "arb_cutoff = 35\n",
    "biprobs=get_probs(xbigrams,xbigramst)\n",
    "monoprobs=get_probs(xonegrams,xonegramst)\n",
    "\n",
    "bigram_prob = [biprobs[b]/(monoprobs[s[0]]*monoprobs[s[1]]) for b,s in zip(unique_biwords,bi_keys_split)]\n",
    "dfbiprob = pd.DataFrame({'biprob':bigram_prob,'bigram':unique_biwords})\n",
    "dfbiprob = dfbiprob.sort('biprob',ascending=False)\n",
    "dfbiprob = dfbiprob[dfbiprob['biprob'] != np.inf]\n",
    "\n",
    "blist=[]\n",
    "for x in dfbiprob['bigram']:\n",
    "    if bi_wc_list[biwordloc[x]] > arb_cutoff:\n",
    "        blist.append(x)\n",
    "        \n",
    "print(blist[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##EXTRA CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hv = HashingVectorizer(norm='l2',stop_words=nltk.corpus.stopwords.words('english'))\n",
    "hvcounts = hv.fit_transform(df['text'])\n",
    "cv = cross_validation.KFold(len(df['stars']), n_folds=10, shuffle=True)\n",
    "params = {'alpha':np.logspace(-6,-3,10)}\n",
    "grid = grid_search.GridSearchCV(linear_model.SGDRegressor(),cv=cv,param_grid=params)\n",
    "grid.fit(hvcounts,df['stars'])\n",
    "\n",
    "with open('/home/vagrant/miniprojects/questions/nlp1.pkl', 'wb') as handle:\n",
    "    pickle.dump(grid, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypipeline3=Pipeline([\n",
    "        ('text_extractor', ColumnSelector('text')),\n",
    "        ('hvect', HashingVectorizer(norm=None, ngram_range=(1, 2), non_negative=True, stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "        ('tfidft', TfidfTransformer()),\n",
    "        ('svd', TruncatedSVD(n_components=100)),\n",
    "        ('normdata', Normalizer(copy=False)),\n",
    "        ('compatibility', Compatibility())\n",
    "])\n",
    "\n",
    "mypipeline2.fit(xfile,yout)\n",
    "\n",
    "with open('/home/vagrant/miniprojects/nlp3.pkl', 'wb') as handle2:\n",
    "    pickle.dump(mypipeline2, handle2)\n",
    "\n",
    "mypipeline2.predict(xfile[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
