{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from collections import namedtuple\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pickle as pickle\n",
    "import time\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC SOCIAL DIARY PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[New York Social Diary](http://www.newyorksocialdiary.com/) provides a fascinating lens onto New York's socially well-to-do.  The data forms a natural social graph for New York's social elite.  Take a look at this page of a recent run-of-the-mill holiday party:\n",
    "\n",
    "`http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers`\n",
    "\n",
    "Almost all the photos have carefully annotated captions labeling their subjects. We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together.\n",
    "\n",
    "In this project, I investigate these connections between the NYC elite.\n",
    "\n",
    "There are two steps -- gathering the data and analyzing it.\n",
    "\n",
    "(1) To gather the data, I grab all the relevant photo-captions and save them; and then parse them to retrieve relevant information.\n",
    "\n",
    "(2) To analyze the data, I consider the problem in terms of a [network](http://en.wikipedia.org/wiki/Computer_network) or a [graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29).  Any time a pair of people appear in a photo together, that is considered a link.  This is an (undirected) [multigraph](http://en.wikipedia.org/wiki/Multigraph) with no self-loops, and has an obvious analog in terms of an undirected [weighted graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Weighted_graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATHERING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to gather the data.  I want photos from parties before December 1st, 2014.\n",
    "`http://www.newyorksocialdiary.com/party-pictures` contains a list of (party) pages. \n",
    "For each party, I find the url, and grab all the photocaptions.\n",
    "\n",
    "(1) As you can see, the url changes are consistent for each party. There is the base url, followed by the year, followed by the party name, with dashes in place of spaces \n",
    "\n",
    "(2) I use python's `datetime.strptime` function to parse the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192\n",
      "/party-pictures/2014/gala-guests\n"
     ]
    }
   ],
   "source": [
    "max_date = \"14/12/01\"\n",
    "max_pages = 25 #actually 24 but just in case\n",
    "url_base = \"http://www.newyorksocialdiary.com/\"\n",
    "url_page_call = \"party-pictures?page=\"\n",
    "\n",
    "cutofftime=datetime.datetime.strptime(max_date, '%y/%m/%d')\n",
    "PicBasic = namedtuple('PicBasic', 'url, dateinfo')\n",
    "\n",
    "def span_info(span):\n",
    "    urldata = span.select('span.field-content > a')\n",
    "    datedata = span.select('span.views-field-created > span.field-content')\n",
    "    if len(urldata)!=1 or len(datedata)!=1:\n",
    "        print \"Uh oh! We did something wrong\"\n",
    "        return None\n",
    "    return PicBasic (\n",
    "            url   = urldata[0]['href'],\n",
    "            dateinfo = datetime.datetime.strptime(datedata[0].text, '%A, %B %d, %Y')\n",
    "            )\n",
    "\n",
    "urladdons=[]\n",
    "for i in range(max_pages):\n",
    "    pageno=i+1\n",
    "    url = url_base + url_page_call + str(pageno)\n",
    "    raw_page = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(raw_page)\n",
    "    t2spans=soup.select('div.views-row')\n",
    "    span_links=[span_info(span) for span in t2spans]\n",
    "    url_links = [datapt.url for datapt in span_links if datapt.dateinfo<cutofftime]\n",
    "    urladdons.extend(url_links) #finds the add-on links\n",
    "\n",
    "print(len(urladdons)) #number of party pages\n",
    "print(urladdons[0]) #add-on url for the first (last chronologically) party we identified\n",
    "\n",
    "all_pic_captions = []\n",
    "max_parties = len(urladdons)\n",
    "\n",
    "def has_class_and_face(tag):\n",
    "    return not tag.has_attr('color') and tag.has_attr('face')\n",
    "\n",
    "for j in range(max_parties):   \n",
    "    try:\n",
    "        soup = BeautifulSoup(urllib2.urlopen(url_base + urladdons[j]))\n",
    "    except: #sometimes the webpage is not responsive; therefore it is necessary to have except statements\n",
    "        try:\n",
    "            soup = BeautifulSoup(urllib2.urlopen(url_base + urladdons[j]))\n",
    "        except:\n",
    "            try:\n",
    "                soup = BeautifulSoup(urllib2.urlopen(url_base + urladdons[j]))\n",
    "            except:\n",
    "                pass\n",
    "    for a in soup.find_all(class_ = \"photocaption\"):\n",
    "        try:\n",
    "            names_with_white = str(a.get_text())\n",
    "            names = names_with_white.lstrip()\n",
    "            all_pic_captions.append(names)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133129\n",
      "132131\n"
     ]
    }
   ],
   "source": [
    "#TAKE OUT THE PHOTOGRAPHER\n",
    "print(len(all_pic_captions))\n",
    "all_pic_captions = [caption for caption in all_pic_captions if not re.search(r'^Photographs by ',caption)]\n",
    "print(len(all_pic_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les Lieberman, Barri Lieberman, Isabel Kallman, Trish Iervolino, and Ron Iervolino \n",
      "132131\n"
     ]
    }
   ],
   "source": [
    "###### SAVE AS PICKLE DATAFRAME FILE ###############\n",
    "####################################################\n",
    "print(all_pic_captions[0])\n",
    "df=pd.DataFrame(all_pic_captions, columns=['all_pic_captions'])\n",
    "df.to_pickle('captions2.pickle')\n",
    "print(len(all_pic_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the parsing part.\n",
    "\n",
    "1. Some captions are not useful: they contain long narrative texts that explain the event.  We have to find some heuristic rules to separate captions that are a list of names from those that are not.  A few heuristics include:\n",
    "      - Looking for sentences (which have verbs) and as opposed to lists of nouns.\n",
    "      - Looking for commonly repeated threads (e.g. you might end up picking up the photo credtis).\n",
    "      - Long captions are often not lists of people.\n",
    "\n",
    "\n",
    "2. I separate the captions based on various forms of punctuation.\n",
    "\n",
    "3. This site is pretty formal and likes to say things like \"Mayor Michael Bloomberg\" after his election but \"Michael Bloomberg\" before his election. There are many titles such as Mayor, CEO, etc, that need to be filtered out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131991"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### OPEN SAVED PICKLE FILE #################\n",
    "###########RUN FROM HERE IF DOING PREVIOUS ANALYSIS#####\n",
    "\n",
    "df=pd.io.pickle.read_pickle('captions2.pickle')\n",
    "allcaptions=df['all_pic_captions']\n",
    "\n",
    "###USE ONLY CAPTIONS UNDER SOME SUBJECTIVE CHARACTER LENGTH\n",
    "subjective_cutoff = 250\n",
    "smallcaps=[caption for caption in allcaptions if len(caption)<subjective_cutoff]\n",
    "len(smallcaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####IDENTIFY VERBS ##########\n",
    "\n",
    "dfiltered=pd.DataFrame(smallcaps, columns=['smallcaps'])\n",
    "\n",
    "capwords = [[re.sub(r'[^\\w\\-\\s]','',word) for word in document.split()]\n",
    "          for document in smallcaps]\n",
    "\n",
    "def extractverbcaps(words):\n",
    "    #function extracts nouns from a tokenized list of unigrams\n",
    "    twords=pos_tag(words)\n",
    "    vtags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    stopvwords=['van','left','right','honoree','de','host','dressed']\n",
    "    verbpresent=0\n",
    "    for word in twords:\n",
    "        if word[1] in vtags:\n",
    "            if not word[0].istitle(): #only check for upper-case words\n",
    "                if word[0] not in stopvwords:\n",
    "                    verbpresent=1        \n",
    "    return verbpresent\n",
    "\n",
    "verbpresent=[extractverbcaps(caption) for caption in capwords]\n",
    "dfiltered['verbpresent']=verbpresent\n",
    "dfiltered['tokenized']=capwords\n",
    "dfiltered.to_pickle('filteredcaptions.pickle') #saving to pickle file\n",
    "filteredcaps=dfiltered[dfiltered['verbpresent'] == 0]['smallcaps']\n",
    "\n",
    "#####GETTING RID OF HONORIFICS, ETC###\n",
    "\n",
    "filteredcaps2= [re.sub(r'[(][a-zA-Z]+[)]','', caps) for caps in filteredcaps] #getting rid of everything inside brackets\n",
    "hwords1=['Mr. ','Guest',' M.D.','PhD','Ph.D.',' Jr.',' Sr.','Mrs. ','Miss ','Doctor ','Dr. ','Dr ','Chair ','CEO ','the Honorable ','Mayor ','Prince ','Baroness ', 'Princess ', 'Honorees ', 'Honoree',' MD']\n",
    "hwordsp=['Museum President ','Chief Curator ','Frick Director ','Police Commissioner ','Music Director ','Frick Trustee ','Historic Hudson Valley Trustee ', 'Museum President ','Public Theater Artistic Director ','Public Theater Executive Director ','Executive Director ','Cooper Union President ','The Hon. ','Dancing Chair ','Director Emerita ']\n",
    "hwords2=['Hon. ','Lord ','Senator ','Deputy ','Director ','Dean ','Actor ','Actress ',' Esq.', 'Gov ','Governor ','Father ','Congresswoman ','Congressman ', 'Countess ','Awardee ','Chairman ','Commissioner ','Lady ','Ambassador ','President ','CEO ']\n",
    "hwords=hwordsp+hwords1+hwords2\n",
    "hwords = '|'.join(list(set(hwords)))\n",
    "filteredcaps2= [re.sub(r'^\\s+|\\s+$','', caps) for caps in filteredcaps2]\n",
    "filteredcaps2= [re.sub(hwords,'', caps) for caps in filteredcaps2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130101\n",
      "\n",
      " WITHOUT REPLACING COUPLES \n",
      "\n",
      "['Melissa Errico, Todd Hollander, and Natalia Bulgari ', 'George Lichtblau, Anne Lichtblau, and Sig Ackerman ', 'Daniel Murphy, Deann Murphy, Jessica Farrell, and Ken Farrell ', 'Jonny Podell ', 'Ashley rmott ', 'Caroline Dean ', 'Jena Silverman, John Silverman, Tess Silverman, and Dara Silverman ', 'Jaime Gleicher and Jared Seligman ', 'Fred Feiner, Ricki Feiner, Sandie Greene, and Bob Greene ', 'Kelly Mulderry, Ted Murphy, and Marci Murphy ', 'Michael Cominotto, Gigi Grimstad, and Dennis Basso ', 'Mark Bessler, Andrea Ziegelman, and Alex Figueroa ', 'Dana and Jamie Creel ', 'Dana Taylor, Baird Ryan, and Alexia Hamm Ryan ', 'Kurt Henckels and Lance Lundeberg ', 'William Candelaria, Mark Brendel, Yaz Hernandez, Dennis Basso, and Giulia Caltagirone ', 'Guy Robinson, Elizabeth Stribling, and Fernanda Kellogg ', 'Eleanor Noell ', 'Melissa Errico ', 'Outside the tent ']\n",
      "\n",
      " REPLACING COUPLES \n",
      "\n",
      "['Melissa Errico, Todd Hollander, and Natalia Bulgari ', 'George Lichtblau, Anne Lichtblau, and Sig Ackerman ', 'Daniel Murphy, Deann Murphy, Jessica Farrell, and Ken Farrell ', 'Jonny Podell ', 'Ashley rmott ', 'Caroline Dean ', 'Jena Silverman, John Silverman, Tess Silverman, and Dara Silverman ', 'Jaime Gleicher and Jared Seligman ', 'Fred Feiner, Ricki Feiner, Sandie Greene, and Bob Greene ', 'Kelly Mulderry, Ted Murphy, and Marci Murphy ', 'Michael Cominotto, Gigi Grimstad, and Dennis Basso ', 'Mark Bessler, Andrea Ziegelman, and Alex Figueroa ', 'Dana Creel and Jamie Creel ', 'Dana Taylor, Baird Ryan, and Alexia Hamm Ryan ', 'Kurt Henckels and Lance Lundeberg ', 'William Candelaria, Mark Brendel, Yaz Hernandez, Dennis Basso, and Giulia Caltagirone ', 'Guy Robinson, Elizabeth Stribling, and Fernanda Kellogg ', 'Eleanor Noell ', 'Melissa Errico ', 'Outside the tent ']\n"
     ]
    }
   ],
   "source": [
    "##########REPLACING COUPLES###########\n",
    "#On investigation, we find that there are a lot of couple names -- i.e. Mary and John Drew.\n",
    "#To parse these, we need to have it in a \"Mary Drew and John Drew\" format.\n",
    "\n",
    "newnames=[]\n",
    "countno=0\n",
    "capstring=\"([A-Z][a-z]+)\\s+and\\s+([A-Z][a-z]+)\\s+([A-Z][a-z]+)\" #string for Kelly and Tom Monro forex\n",
    "begstring=\"^%s\" % capstring #string if it appears in the beginning\n",
    "andstring=\"\\\\s+and\\\\s+%s\" % capstring\n",
    "withstring=\"\\\\s+with\\\\s+%s\" % capstring\n",
    "otherstring=\"\\\\s+[a-z]+\\\\s+%s\" % capstring\n",
    "\n",
    "def findingpairs(xlistno):\n",
    "    namestr=[]\n",
    "    for names in xlistno:\n",
    "        nstr = names[0] + \" \" + names[2] + \" and \" + names[1] + \" \" + names[2]\n",
    "        namestr.append(nstr)\n",
    "    return(', '.join(namestr))\n",
    "\n",
    "for xnames in filteredcaps2:\n",
    "    xlistno2=re.search(otherstring,xnames)\n",
    "    xlistno=re.search(begstring,xnames)\n",
    "    if xlistno2:\n",
    "        xno= re.findall(capstring,xnames)\n",
    "        if len(xno)>1:\n",
    "            xn=findingpairs(xno)\n",
    "            newnames.append(xn)\n",
    "        else:\n",
    "            newstring=xlistno2.group(1)+ \" \" + xlistno2.group(3) + \" and \" + xlistno2.group(2) + \" \" + xlistno2.group(3)\n",
    "            newnames.append(re.sub(capstring, newstring, xnames))\n",
    "    elif xlistno:       \n",
    "        xno= re.findall(capstring,xnames)\n",
    "        if len(xno)>1:\n",
    "            xn=findingpairs(xno)\n",
    "            newnames.append(xn)\n",
    "        else:\n",
    "            newstring=xlistno.group(1)+ \" \" + xlistno.group(3) + \" and \" + xlistno.group(2) + \" \" + xlistno.group(3)\n",
    "            newnames.append(re.sub(capstring, newstring, xnames))\n",
    "    else:\n",
    "        newnames.append(xnames)\n",
    "\n",
    "print(len(newnames))\n",
    "print(\"\\n WITHOUT REPLACING COUPLES \\n\")\n",
    "print(filteredcaps2[30:50])\n",
    "print(\"\\n REPLACING COUPLES \\n\")\n",
    "print(newnames[30:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Kelly Mulderry', 'Ted Murphy', 'Marci Murphy'], ['Michael Cominotto', 'Gigi Grimstad', 'Dennis Basso'], ['Mark Bessler', 'Andrea Ziegelman', 'Alex Figueroa'], ['Dana Creel', 'Jamie Creel'], ['Dana Taylor', 'Baird Ryan', 'Alexia Hamm Ryan'], ['Kurt Henckels', 'Lance Lundeberg'], ['William Candelaria', 'Mark Brendel', 'Yaz Hernandez', 'Dennis Basso', 'Giulia Caltagirone'], ['Guy Robinson', 'Elizabeth Stribling', 'Fernanda Kellogg'], ['Jenny Price', 'Sharon Jacob', 'Stephanie Shuman', 'Gillian Miniter', 'Deborah Roberts'], ['Cynthia Conway', 'Tom Kempner', 'Luann Blowers', 'David Blowers'], ['Michael Price', 'Jenny Price'], ['Bob Cochran', 'Suzanne Cochran'], ['Patsy Tarr', 'Jeff Tarr'], ['Fred Shuman', 'Stephanie Shuman'], ['Gillian Miniter', 'Sylvester Miniter'], ['Iffie Okoronkwo Aitkenhead', 'Agenia Clark', 'Deborah Roberts', 'Fiona Rudin'], ['Michael Evans', 'Lise Evans'], ['Fe Fendi', 'Alessandro Fendi'], ['Jill Ross', 'Sharon Teles', 'Eby McKay'], ['Kathryn Chenault', 'Carol Sutton Lewis']]\n"
     ]
    }
   ],
   "source": [
    "## FURTHER PARSING TO GET IN LIST OF NAMES FORMAT ##\n",
    "\n",
    "newnames2 = [re.split(r',\\s+and\\s+|,\\s+with\\s+|;\\s|\\s+and\\s+|\\s+amd\\s+|,\\s|\\s+with\\s+',mylistentries) for mylistentries in newnames]            \n",
    "nameslist = [[word for word in caps if word !='']\n",
    "             for caps in newnames2]\n",
    "nameslist=[[re.sub(r'\\s+$|^\\s+|\\s+\\n|\\n\\s+|\\n','', caps) for caps in names]\n",
    "       for names in nameslist]\n",
    "nameslist=[names for names in nameslist if len(names)>1]\n",
    "nameslist=[[caps for caps in names if names[0].istitle()]\n",
    "       for names in nameslist]\n",
    "stopwords=['friend','her daughter','President','CEO','Hospital for Special Surgery', 'a friend','NYU','son','sons','wife','dean','daughters','friends','guest','Guest','children','daughter','his wife','squires','guests','family','left','right','presents','welcomes','honoree','host']\n",
    "nameslist = [[names for names in nameinds if names not in stopwords]\n",
    "          for nameinds in nameslist]\n",
    "print(nameslist[30:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYZING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) A simple question we can ask is 'who is the most popular'?  The easiest way to answer this question is to look at how many connections everyone has -- returning the top 100 people and their degree.\n",
    "\n",
    "(2) A similar way to determine popularity is to look at their [pagerank](http://en.wikipedia.org/wiki/PageRank).  Pagerank is essentially the [stationary distribution](http://en.wikipedia.org/wiki/Markov_chain#Stationary_distribution_relation_to_eigenvectors_and_simplices) of a [markov chain](http://en.wikipedia.org/wiki/Markov_chain) implied by the social graph.\n",
    "\n",
    "(3) Another interesting question is who tend to co-occur with each other. We might even be able to use this analysis to detect instances of affairs and infidelities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## FIRST WE ENTER THE DATA INTO GRAPH FORMAT (i.e., containing edges and nodes)\n",
    "\n",
    "def joinlists(listname):\n",
    "    #function for joining list of lists (i.e. from x=([['a','b'],['c']]) to x=['a','b','c'])\n",
    "    #want only unique values for each document, so:\n",
    "    uniquelitems = [set(listitems) for listitems in listname]\n",
    "    newlist=list(itertools.chain.from_iterable(uniquelitems))\n",
    "    return newlist\n",
    "\n",
    "tot_allwords=joinlists(nameslist)\n",
    "uniquenames=list(set(tot_allwords))\n",
    "\n",
    "edgelists = [sorted(captions) for captions in nameslist]\n",
    "edgelists = [itertools.combinations(captions,2) for captions in edgelists]\n",
    "edgelists = [list(captions) for captions in edgelists]\n",
    "edgelists = sum(edgelists, [])\n",
    "\n",
    "uniquedges=list(set(edgelists))\n",
    "\n",
    "xedges=Counter(edgelists)\n",
    "edgecounts=[xedges[namestr] for namestr in uniquedges]\n",
    "\n",
    "wedges=[0]*len(edgecounts)\n",
    "for i in range(len(edgecounts)):\n",
    "    wedges[i]=(uniquedges[i][0],uniquedges[i][1],edgecounts[i])\n",
    "    \n",
    "G=nx.Graph()\n",
    "G.add_nodes_from(uniquenames)\n",
    "G.add_weighted_edges_from(wedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST POPULAR PEOPLE IN THE NYC SOCIAL SCENE \\N\n",
      "('Jean Shafiroff', 346)\n",
      "('Gillian Miniter', 309)\n",
      "('Mark Gilbertson', 298)\n",
      "('Andrew Saffir', 204)\n",
      "('Geoffrey Bradfield', 200)\n",
      "('Alexandra Lebenthal', 196)\n",
      "('Somers Farkas', 190)\n",
      "('Debbie Bancroft', 166)\n",
      "('Jamee Gregory', 166)\n",
      "('Yaz Hernandez', 154)\n",
      "('Sharon Bush', 153)\n",
      "('Eleanora Kennedy', 148)\n",
      "('Sylvester Miniter', 143)\n",
      "('Mario Buatta', 141)\n",
      "('Barbara Tober', 141)\n",
      "('Alina Cho', 136)\n",
      "('Kamie Lightburn', 131)\n",
      "('Bonnie Comley', 126)\n",
      "('Muffie Potter Aston', 125)\n",
      "('Daniel Benedict', 124)\n",
      "('Bettina Zilkha', 122)\n",
      "('Liliana Cavendish', 119)\n",
      "('Amy Fine Collins', 112)\n",
      "('Barbara Regna', 111)\n",
      "('Grace Meigher', 109)\n",
      "('Lucia Hwong Gordon', 109)\n",
      "('Michael Bloomberg', 106)\n",
      "('Amy Hoadley', 101)\n",
      "('Allison Aston', 101)\n",
      "('Margo Langenberg', 100)\n",
      "('Stewart Lane', 99)\n",
      "('Liz Peek', 98)\n",
      "('Roric Tobin', 97)\n",
      "('Nicole Miller', 97)\n",
      "('Dennis Basso', 96)\n",
      "('Jennifer Creel', 95)\n",
      "('R. Couri Hay', 93)\n",
      "('Peter Gregory', 90)\n",
      "('Donna Karan', 90)\n",
      "('Fe Fendi', 89)\n",
      "('Janna Bullock', 89)\n",
      "('Lydia Fenet', 88)\n",
      "('Deborah Norville', 87)\n",
      "('Martha Stewart', 87)\n",
      "('Felicia Taylor', 85)\n",
      "('Coco Kopelman', 84)\n",
      "('Audrey Gruss', 84)\n",
      "('Lizzie Tisch', 83)\n",
      "('John Demsey', 82)\n",
      "('Karen Klopp', 81)\n",
      "('Jonathan Farkas', 81)\n",
      "('Wendy Carduner', 81)\n",
      "('Frederick Anderson', 80)\n",
      "('Diana Taylor', 80)\n",
      "('Wilbur Ross', 80)\n",
      "('Hilary Geary Ross', 80)\n",
      "('Cynthia Lufkin', 79)\n",
      "('Hunt Slonem', 78)\n",
      "('Douglas Hannant', 78)\n",
      "('Russell Simmons', 77)\n",
      "('Tory Burch', 77)\n",
      "('Fernanda Kellogg', 77)\n",
      "('Evelyn Lauder', 75)\n",
      "('Ann Rapp', 75)\n",
      "('Alexandra Lind Rose', 74)\n",
      "('Patricia Shiah', 74)\n",
      "('Alec Baldwin', 74)\n",
      "('Michele Gerber Klein', 73)\n",
      "('Kipton Cronkite', 73)\n",
      "('Elizabeth Stribling', 73)\n",
      "('Leonard Lauder', 73)\n",
      "('Fern Mallis', 72)\n",
      "('Jonathan Tisch', 72)\n",
      "('Donald Tober', 72)\n",
      "('Anka Palitz', 72)\n",
      "('Nina Griscom', 71)\n",
      "('Adelina Wong Ettelson', 71)\n",
      "('Lisa Anastos', 71)\n",
      "('Campion Platt', 70)\n",
      "('Richard Johnson', 70)\n",
      "('Mary Snow', 70)\n",
      "('Steven Stolman', 70)\n",
      "('Alison Minton', 69)\n",
      "('Susan Shin', 69)\n",
      "('Julia Koch', 69)\n",
      "('Heather Leeds', 68)\n",
      "('Zani Gugelmann', 68)\n",
      "('Michael Kennedy', 68)\n",
      "('Melissa Berkelhammer', 68)\n",
      "('Melissa Morris', 67)\n",
      "('Michele Herbert', 67)\n",
      "('Martin Shafiroff', 67)\n",
      "('Peter Davis', 66)\n",
      "('Carol Mack', 66)\n",
      "('Mary Davidson', 66)\n",
      "('Mary Van Pelt', 66)\n",
      "('Tinsley Mortimer', 65)\n",
      "('Christine Schwarzman', 65)\n",
      "('David Koch', 65)\n",
      "('Cassandra Seidenfeld', 65)\n"
     ]
    }
   ],
   "source": [
    "## 1: DETERMINING MOST POPULAR NAMES THAT APPEAR IN THE PHOTO CAPTIONS\n",
    "\n",
    "deg_names = G.degree(weight='weight')\n",
    "deg_names_sorted = sorted(deg_names.items(), key=operator.itemgetter(1),reverse=True)\n",
    "deg_half = [(d[0],d[1]/2) for d in deg_sort]\n",
    "print(\"MOST POPULAR PEOPLE IN THE NYC SOCIAL SCENE \\N\")\n",
    "for x in range(100):\n",
    "    print(deg_sort[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST INFLUENTIAL PEOPLE IN THE NYC SOCIAL SCENE: \n",
      "\n",
      "('Jean Shafiroff', 0.0006680686790028176)\n",
      "('Mark Gilbertson', 0.0005077194888412853)\n",
      "('Gillian Miniter', 0.00043659950484159607)\n",
      "('Geoffrey Bradfield', 0.00036226295504544)\n",
      "('Andrew Saffir', 0.0003462215665377449)\n",
      "('Alexandra Lebenthal', 0.00033723551290372147)\n",
      "('Sharon Bush', 0.00031230979335758653)\n",
      "('Yaz Hernandez', 0.00030702334845351704)\n",
      "('Somers Farkas', 0.0003005394065802728)\n",
      "('Mario Buatta', 0.0002959690065681298)\n",
      "('Debbie Bancroft', 0.00028454491381753913)\n",
      "('Eleanora Kennedy', 0.0002823685240168033)\n",
      "('Kamie Lightburn', 0.00027685818151433465)\n",
      "('Barbara Tober', 0.0002725415737560136)\n",
      "('Alina Cho', 0.00026386352073388665)\n",
      "('Michael Bloomberg', 0.0002589816846743657)\n",
      "('Bonnie Comley', 0.0002516745738431008)\n",
      "('Lucia Hwong Gordon', 0.00024987602625568724)\n",
      "('Liliana Cavendish', 0.0002383901042492601)\n",
      "('Christopher Hyland', 0.00023245754368317318)\n",
      "('Jamee Gregory', 0.00022636685338120333)\n",
      "('Muffie Potter Aston', 0.00022235854477034835)\n",
      "('Martha Stewart', 0.00021902650189749152)\n",
      "('Bettina Zilkha', 0.00021548717276869715)\n",
      "('Lydia Fenet', 0.00020891822839858403)\n",
      "('Amy Fine Collins', 0.00020607001268067306)\n",
      "('Stewart Lane', 0.00019883507675542297)\n",
      "('Russell Simmons', 0.00019802569726042074)\n",
      "('Daniel Benedict', 0.0001949245621619272)\n",
      "('Diana Taylor', 0.00019298611569804107)\n",
      "('Allison Aston', 0.00019294247860226677)\n",
      "('Fernanda Kellogg', 0.0001927206331957139)\n",
      "('Kipton Cronkite', 0.0001871287254486088)\n",
      "('Evelyn Lauder', 0.00018616341752590565)\n",
      "('Barbara Regna', 0.0001843308394930202)\n",
      "('Karen Klopp', 0.00018213138936168635)\n",
      "('Margo Langenberg', 0.00018117065144171237)\n",
      "('Donna Karan', 0.00018112227009216478)\n",
      "('Steven Stolman', 0.00018062099166177376)\n",
      "('Grace Meigher', 0.00017959073080158245)\n",
      "('Elizabeth Stribling', 0.00017673695268068394)\n",
      "('Dawne Marie Grannum', 0.00017648339904248147)\n",
      "('Dennis Basso', 0.00017600181187875388)\n",
      "('Leonard Lauder', 0.0001753201700144135)\n",
      "('Liz Peek', 0.00017370700719647236)\n",
      "('Michele Herbert', 0.00017327635659803497)\n",
      "('Amy Hoadley', 0.00017324772281271408)\n",
      "('Roric Tobin', 0.0001719893596780435)\n",
      "('Anka Palitz', 0.00017182286485233932)\n",
      "('Janna Bullock', 0.00017135674132351603)\n",
      "('Rosanna Scotto', 0.00016952556970730492)\n",
      "('R. Couri Hay', 0.00016920241659900326)\n",
      "('Felicia Taylor', 0.00016895255094268334)\n",
      "('Deborah Norville', 0.00016881388184446081)\n",
      "('Nicole Miller', 0.0001670932844622423)\n",
      "('Alec Baldwin', 0.00016441264939403693)\n",
      "('Audrey Gruss', 0.00016297902526761748)\n",
      "('Jennifer Creel', 0.00016245524993493195)\n",
      "('Annette Rickel', 0.00015941303411115087)\n",
      "('Paula Zahn', 0.00015879578171048282)\n",
      "('Fern Mallis', 0.00015741360333393936)\n",
      "('Michele Gerber Klein', 0.00015631576057699046)\n",
      "('Fe Fendi', 0.00015617102143334713)\n",
      "('Tory Burch', 0.00015582118204567142)\n",
      "('Lisa Anastos', 0.000154181667443866)\n",
      "('Richard Johnson', 0.00015237315076600368)\n",
      "('Agnes Gund', 0.00015217060292409824)\n",
      "('Sylvester Miniter', 0.00015179142904707288)\n",
      "('Coco Kopelman', 0.00015030581458566337)\n",
      "('Tinsley Mortimer', 0.00014866621135185547)\n",
      "('Georgina Schaeffer', 0.0001482253630423582)\n",
      "('Mary Van Pelt', 0.0001465700067103552)\n",
      "('Bette Midler', 0.00014586426288570796)\n",
      "('John Demsey', 0.0001456267355440231)\n",
      "('Chuck Scarborough', 0.00014456396989738802)\n",
      "('Adelina Wong Ettelson', 0.00014434272947252462)\n",
      "('Cassandra Seidenfeld', 0.00014313529172450476)\n",
      "('Wendy Carduner', 0.00014124462501137862)\n",
      "('Lizzie Tisch', 0.00014052015383866844)\n",
      "('Margo Catsimatidis', 0.00013865172330918503)\n",
      "('Patricia Shiah', 0.0001386446942319718)\n",
      "('Barbara Walters', 0.00013795329999782594)\n",
      "('Susan Shin', 0.00013764057137683833)\n",
      "('Kristian Laliberte', 0.00013748394874709126)\n",
      "('Pamela Fiori', 0.00013719499957558134)\n",
      "('Heather Leeds', 0.00013704950453041138)\n",
      "('Mary Davidson', 0.00013654426822036177)\n",
      "('Fabiola Beracasa', 0.000135056647210653)\n",
      "('Melissa Berkelhammer', 0.00013284832536515127)\n",
      "('Nathalie Kaplan', 0.0001324303067030337)\n",
      "('Gregory Long', 0.00013195622699712294)\n",
      "('Bettina Prentice', 0.00013153452241230942)\n",
      "('Marc Rosen', 0.00013118671814301306)\n",
      "('Edward Callaghan', 0.0001309615769931228)\n",
      "('Cynthia Lufkin', 0.00013093269112924612)\n",
      "('Hunt Slonem', 0.0001303575535510108)\n",
      "('Tina Brown', 0.00013015032931930074)\n",
      "('Amy Phelan', 0.00012999603775848927)\n",
      "('John Wegorzewski', 0.00012995965758374122)\n",
      "('Deborah Roberts', 0.00012983077119543116)\n"
     ]
    }
   ],
   "source": [
    "## 2: DETERMINING TOP 100 MOST INFLUENTIAL PEOPLE IN THE NYC SOCIAL SCENE\n",
    "\n",
    "pagerankout=[]\n",
    "pgpop=nx.pagerank(G, alpha=0.85, personalization=None, max_iter=100)\n",
    "highest = sorted(pgpop, key=pgpop.get, reverse=True)\n",
    "\n",
    "for eachname in highest[0:100]:\n",
    "    x=(eachname,pgpop[eachname])\n",
    "    pagerankout.append(x)\n",
    "\n",
    "#saving to pickle file\n",
    "output = open('../../miniprojects/questions/pagerank2.pickle','w')\n",
    "pickle.dump(pagerankout,output)\n",
    "output.close()\n",
    "\n",
    "#sorted_by_pagerank=pickle.load(open('../../miniprojects/questions/pagerank2.pickle'))\n",
    "print(\"MOST INFLUENTIAL PEOPLE IN THE NYC SOCIAL SCENE: \\n\")\n",
    "#for sorted_people in sorted_by_pagerank:\n",
    "for sorted_people in pagerankout:\n",
    "    print(sorted_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People that appear together most frequently in pictures: \n",
      "\n",
      "(('Sylvester Miniter', 'Gillian Miniter'), 123)\n",
      "(('Bonnie Comley', 'Stewart Lane'), 82)\n",
      "(('Peter Gregory', 'Jamee Gregory'), 77)\n",
      "(('Andrew Saffir', 'Daniel Benedict'), 68)\n",
      "(('Geoffrey Bradfield', 'Roric Tobin'), 66)\n",
      "(('Donald Tober', 'Barbara Tober'), 58)\n",
      "(('Somers Farkas', 'Jonathan Farkas'), 57)\n",
      "(('Jean Shafiroff', 'Martin Shafiroff'), 56)\n",
      "(('Eleanora Kennedy', 'Michael Kennedy'), 50)\n",
      "(('Jay Diamond', 'Alexandra Lebenthal'), 49)\n",
      "(('Campion Platt', 'Tatiana Platt'), 48)\n",
      "(('Yaz Hernandez', 'Valentin Hernandez'), 48)\n",
      "(('Jonathan Tisch', 'Lizzie Tisch'), 45)\n",
      "(('Grace Meigher', 'Chris Meigher'), 43)\n",
      "(('Melissa Morris', 'Chappy Morris'), 43)\n",
      "(('Barbara Regna', 'Peter Regna'), 42)\n",
      "(('Deborah Norville', 'Karl Wellner'), 40)\n",
      "(('Margo Catsimatidis', 'John Catsimatidis'), 40)\n",
      "(('Hilary Geary Ross', 'Wilbur Ross'), 35)\n",
      "(('Elizabeth Stribling', 'Guy Robinson'), 35)\n",
      "(('Frederick Anderson', 'Douglas Hannant'), 34)\n",
      "(('David Koch', 'Julia Koch'), 34)\n",
      "(('Coco Kopelman', 'Arie Kopelman'), 32)\n",
      "(('R. Couri Hay', 'Janna Bullock'), 31)\n",
      "(('Clo Cohen', 'Charles Cohen'), 30)\n",
      "(('Fernanda Kellogg', 'Kirk Henckels'), 30)\n",
      "(('Michael Cominotto', 'Dennis Basso'), 29)\n",
      "(('Leonel Piraino', 'Nina Griscom'), 28)\n",
      "(('Sessa von Richthofen', 'Richard Johnson'), 27)\n",
      "(('Olivia Palermo', 'Johannes Huebl'), 27)\n",
      "(('Chuck Scarborough', 'Ellen Scarborough'), 26)\n",
      "(('Dan Lufkin', 'Cynthia Lufkin'), 26)\n",
      "(('Arlene Dahl', 'Marc Rosen'), 25)\n",
      "(('Tommy Hilfiger', 'Dee Ocleppo'), 25)\n",
      "(('Mark Badgley', 'James Mischka'), 25)\n",
      "(('Melania Trump', 'Donald Trump'), 25)\n",
      "(('Liz Peek', 'Jeff Peek'), 24)\n",
      "(('David Lauren', 'Lauren Bush'), 23)\n",
      "(('Alina Cho', 'John Demsey'), 23)\n",
      "(('Donna Soloway', 'Richard Soloway'), 23)\n",
      "(('John Wambold', 'Melanie Wambold'), 23)\n",
      "(('Wilbur Ross', 'Hilary Ross'), 23)\n",
      "(('Judy Gilbert', 'Rod Gilbert'), 22)\n",
      "(('Al Roker', 'Deborah Roberts'), 22)\n",
      "(('Jean Shafiroff', 'Patricia Shiah'), 22)\n",
      "(('Othon Prounis', 'Kathy Prounis'), 22)\n",
      "(('Sherrell Aston', 'Muffie Potter Aston'), 22)\n",
      "(('Stephanie Krieger', 'Brian Stewart'), 21)\n",
      "(('Sharon Bush', 'Jean Shafiroff'), 21)\n",
      "(('Marvin Davidson', 'Mary Davidson'), 21)\n",
      "(('Diana Taylor', 'Michael Bloomberg'), 21)\n",
      "(('Anna Safir', 'Eleanora Kennedy'), 21)\n",
      "(('Richard Steinberg', 'Renee Steinberg'), 20)\n",
      "(('Hunt Slonem', 'Liliana Cavendish'), 20)\n",
      "(('Laura Slatkin', 'Harry Slatkin'), 20)\n",
      "(('Elaine Langone', 'Ken Langone'), 20)\n",
      "(('Coleman Burke', 'Susan Burke'), 19)\n",
      "(('Keytt Lundqvist', 'Alex Lundqvist'), 19)\n",
      "(('Naeem Khan', 'Ranjana Khan'), 19)\n",
      "(('Rick Hilton', 'Kathy Hilton'), 19)\n",
      "(('Isabel Toledo', 'Ruben Toledo'), 19)\n",
      "(('Somers Farkas', 'Muffie Potter Aston'), 19)\n",
      "(('Heather Leeds', 'Tom Leeds'), 18)\n",
      "(('Tony Ingrao', 'Randy Kemper'), 18)\n",
      "(('Bobby Zarin', 'Jill Zarin'), 18)\n",
      "(('Howard Sobel', 'Gayle Sobel'), 18)\n",
      "(('Marcia Mishaan', 'Richard Mishaan'), 17)\n",
      "(('Alexandra Lebenthal', 'Claudia Lebenthal'), 17)\n",
      "(('Marcy Warren', 'Michael Warren'), 17)\n",
      "(('Serena Miniter', 'Gillian Miniter'), 17)\n",
      "(('Cece Black', 'Lee Black'), 17)\n",
      "(('Geoffrey Thomas', 'Sharon Sondes'), 17)\n",
      "(('Gillian Hearst Simonds', 'Christian Simonds'), 17)\n",
      "(('Nicole Miller', 'Kim Taipale'), 17)\n",
      "(('Thorne Perkin', 'Tatiana Perkin'), 17)\n",
      "(('Bunny Williams', 'John Rosselli'), 16)\n",
      "(('Chuck Royce', 'Deborah Royce'), 16)\n",
      "(('Harry Kargman', 'Jill Kargman'), 16)\n",
      "(('Mario Singer', 'Ramona Singer'), 16)\n",
      "(('Ann Rapp', 'Roy Kean'), 16)\n",
      "(('Marisol Thomas', 'Rob Thomas'), 16)\n",
      "(('Shirin von Wulffen', 'Frederic Fekkai'), 16)\n",
      "(('Philip Gorrivan', 'Lisa Gorrivan'), 16)\n",
      "(('Daniel Benedict', 'Johannes Huebl'), 16)\n",
      "(('Robert Bradford', 'Barbara Taylor Bradford'), 16)\n",
      "(('Debbie Bancroft', 'Tiffany Dubin'), 15)\n",
      "(('Arnie Rosenshein', 'Paola Rosenshein'), 15)\n",
      "(('Charlotte Ronson', 'Ali Wise'), 15)\n",
      "(('Whitney Fairchild', 'James Fairchild'), 15)\n",
      "(('Roxanne Palin', 'Dean Palin'), 15)\n",
      "(('Alexandra Lebenthal', 'Gillian Miniter'), 15)\n",
      "(('Anna Wintour', 'Bee Shaffer'), 15)\n",
      "(('Matt Semino', 'Linette Semino'), 15)\n",
      "(('Mary Snow', 'Ian Snow'), 15)\n",
      "(('Will Cotton', 'Rose Dergan'), 15)\n",
      "(('Susan Magazine', 'Nicholas Scoppetta'), 15)\n",
      "(('Samantha Yanks', 'David Yanks'), 15)\n",
      "(('Larry Wohl', 'Leesa Rowland'), 14)\n",
      "(('Ken Starr', 'Diane Passage'), 14)\n",
      "(('Edward Callaghan', 'John Wegorzewski'), 14)\n"
     ]
    }
   ],
   "source": [
    "## 3: DETERMINING TOP CONNECTIONS\n",
    "\n",
    "edge_list = G.edges(data=True)\n",
    "edge_sort = sorted(edge_list, key=operator.itemgetter(2),reverse=True)\n",
    "edge_mod = [((a[0],a[1]),a[2]['weight']) for a in edge_sort]\n",
    "print(\"People that appear together most frequently in pictures: \\n\")\n",
    "for x in range(100):\n",
    "    print(edge_mod[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
